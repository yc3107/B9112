{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "web_crawler_stack.py\n",
    "    - The web crawler with a stack.\n",
    "@author: Dongwook Shin and Yash Kanoria, 2014/08/14\n",
    "@author: Kriste Krstovski, 2018/08/20 (edits to reflect Python 3 version)\n",
    "This web crawler walks through the URLs in the source website and keep \n",
    "crawling until there is no URL to be scanned. It stops crawling if a preset \n",
    "maximum number of urls have been visited.\n",
    "\n",
    "\"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "import re, urllib.parse, urllib.request\n",
    "\n",
    "url = \"http://www8.gsb.columbia.edu\"\n",
    "\n",
    "urls = [url]    #queue of urls to crawl\n",
    "seen = [url] # stack of urls seen so far\n",
    "nUrl = 0;\n",
    "maxNumUrl = 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting with url=\"+str(urls))\n",
    "while len(urls) > 0 and nUrl < maxNumUrl:\n",
    "    # DEQUEUE A URL FROM urls AND TRY TO OPEN AND READ IT\n",
    "    try:\n",
    "        curr_url=urls.pop(0)\n",
    "        print(\"num. of URLs in stack: %d \" % len(urls))\n",
    "        webpage=urllib.request.urlopen(curr_url)\n",
    "        nUrl += 1\n",
    "    except Exception as ex: #if urlopen() fails\n",
    "        print(ex)\n",
    "        continue    #skip code below\n",
    "\n",
    "    # IF URL OPENS, CHECK WHICH URLS THE PAGE CONTAINS\n",
    "    # ADD THE URLS FOUND TO THE QUEUE url AND seen\n",
    "    soup = BeautifulSoup(webpage)  #creates object soup\n",
    "    # Put child URLs into the stack\n",
    "    for tag in soup.find_all('a', href = True): #find tags with links\n",
    "        childUrl = tag['href']          #extract just the link\n",
    "        childUrl = urllib.parse.urljoin(url, childUrl)\n",
    "        if url in childUrl and childUrl not in seen:\n",
    "            urls.append(childUrl)\n",
    "            seen.append(childUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"num. of URLs seen = %d, and scanned = %d\" % (len(seen), nUrl))\n",
    "print(\"List of seen URLs:\")\n",
    "for seen_url in seen:\n",
    "    print(seen_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
